{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/jahnavis/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from epitome.models import *\n",
    "from epitome.functions import *\n",
    "from epitome.viz import *\n",
    "\n",
    "from epitome.constants import *\n",
    "import yaml\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "epitome_data_path = \"data/epitome_data\" \n",
    "feature_path = os.path.join(epitome_data_path, \"feature_name\")\n",
    "TF = \"USF2\"\n",
    "query_cell = 'K562' #'T47D'\n",
    "prefix = \"cons_dec\" # consecutive decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user directories if they do not exist\n",
    "epitome_results_dir = os.path.join(results_path, \"epitome_results\")\n",
    "if not os.path.exists(epitome_results_dir):\n",
    "    os.makedirs(epitome_results_dir)\n",
    "    \n",
    "tf_epitome_results_dir = os.path.join(epitome_results_dir, TF + \"_\" + prefix + \"_results\")\n",
    "if not os.path.exists(tf_epitome_results_dir):\n",
    "    os.makedirs(tf_epitome_results_dir)\n",
    "    \n",
    "model_dir = os.path.join(results_path, \"epitome_models\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "tf_model_dir = os.path.join(model_dir, TF +  \"_\" + prefix + \"_models\")\n",
    "if not os.path.exists(tf_model_dir):\n",
    "    os.makedirs(tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data for Epitome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'train.npz')).toarray()\n",
    "valid_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'valid.npz')).toarray()\n",
    "test_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'test.npz')).toarray()\n",
    "data = {Dataset.TRAIN: train_data, Dataset.VALID: valid_data, Dataset.TEST: test_data}\n",
    "# all_data = np.concatenate((data[Dataset.TRAIN], data[Dataset.VALID], data[Dataset.TEST]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model with Multiple TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_overlap_tfs = pd.read_csv(\n",
    "    \"/home/eecs/jahnavis/epitome_new/epitome-1/data/epitome_data/Anchor_Epitome_Overlap_TFs.csv\")['TF'].tolist()\n",
    "anchor_tfs = [\"CTCF\", \"E2F1\", \"EGR1\", \"FOXA1\", \"FOXA2\", \"GABPA\", \"HNF4A\", \"JUND\", \n",
    "              \"MAX\", \"NANOG\", \"REST\", \"TAF1\"]\n",
    "# anchor_overlap_tfs = set(epitome_tfs).intersection(set(anchor_tfs))\n",
    "# len(anchor_tfs), len(anchor_overlap_tfs), anchor_overlap_tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = anchor_overlap_tfs,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n",
    "VLP(anchor_overlap_tfs,\n",
    "    data = data,\n",
    "    matrix = matrix,\n",
    "    cellmap = cellmap,\n",
    "    assaymap = assaymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFs = [\"CEBPB\", \"CHD2\", \"CTCF\", \"EP300\", \"GABPA\", \"JUND\", \"MAFK\", \"MAX\", \n",
    "#        \"MYC\", \"NRF1\", \"RAD21\", \"REST\", \"RFX5\", \"SRF\", \"TAF1\", \"TBP\", \"USF2\"]\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path)\n",
    "#                                                          eligible_assays = TFs, \n",
    "#                                                          eligible_cells = None, \n",
    "#                                                          min_cells_per_assay = 2)\n",
    "# VLP(TFs,\n",
    "#     data = data,\n",
    "#     matrix = matrix,\n",
    "#     cellmap = cellmap,\n",
    "#     assaymap = assaymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_assays = [a for a in list(assaymap)]\n",
    "label_assays = [a for a in feature_assays if a not in [\"DNase\"]]\n",
    "indices = np.concatenate([get_y_indices_for_assay(matrix, assaymap, assay) for assay in label_assays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model with Early Stopping (Single TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_valid_iterations= 1000\n",
    "\n",
    "model = VLP(assays=[\"CTCF\"], \n",
    "            assaymap=assaymap, \n",
    "            cellmap=cellmap, \n",
    "            matrix=matrix,\n",
    "            max_valid_records=max_valid_iterations)\n",
    "\n",
    "start = timer()\n",
    "model_checkpoint_path = os.path.join(\"test_path\")\n",
    "best_iters_trained, actual_iters_trained, valid_losses = model.train(200, \n",
    "                                                                     patience=2, \n",
    "                                                                     min_delta=0.1)\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "# model_path = os.path.join(tf_model_dir, TF + \"_early_stop_\" + str(iters_trained) + \"_\" + str(max_valid_iterations))\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model.test(200, calculate_metrics=True)\n",
    "# print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_model = VLP(checkpoint=model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_results = model_checkpoint_model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_checkpoint_results['auROC'], model_checkpoint_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['valid_losses'])\n",
    "eval_results_df = eval_results_df.append({ 'valid_losses':valid_losses}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(valid_losses).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['transcription_factor', 'query_cell', 'auROC', 'auPRC'])\n",
    "eval_results_df = eval_results_df.append({ \n",
    "   'transcription_factor' : TF,\n",
    "   'query_cell' : query_cell,\n",
    "   'auROC' : model_results['auROC'],\n",
    "   'auPRC' : model_results['auPRC'],\n",
    "   'iterations_trained' : iter_trained,\n",
    "   'train_time': train_time}, \n",
    "    ignore_index=True)\n",
    "\n",
    "eval_results_dir = os.path.join(tf_epitome_results_dir, query_cell + \"_\" + TF + \n",
    "                           '_no_motif_early_stop_'+ str(max_valid_iterations) + \n",
    "                           '.csv')\n",
    "eval_results_df.to_csv(eval_results_dir, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "#                                                          eligible_assays = None,\n",
    "#                                                          eligible_cells = None, \n",
    "#                                                          min_cells_per_assay = 2, \n",
    "#                                                          min_assays_per_cell= 2) #10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = \"JUND\"\n",
    "model = VLP([TF])\n",
    "\n",
    "start = timer()\n",
    "iter_trained = model.train(500) # train for 5000 iterations\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "\n",
    "model_results = model.test(1000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['transcription_factor', 'query_cell', 'auROC', 'auPRC'])\n",
    "eval_results_df = eval_results_df.append({ \n",
    "   'transcription_factor' : TF,\n",
    "   'query_cell' : query_cell,\n",
    "   'auROC' : model_results['auROC'],\n",
    "   'auPRC' : model_results['auPRC'],\n",
    "   'iterations_trained' : iter_trained,\n",
    "   'train_time': train_time}, \n",
    "    ignore_index=True)\n",
    "\n",
    "eval_results_df.to_csv(os.path.join(tf_epitome_results_dir,\n",
    "                                    query_cell + \"_\" + TF + '_no_motif' + '.csv'), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['preds_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = \"JUND\"\n",
    "\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = TF,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n",
    "    \n",
    "model2 = VLP([TF],\n",
    "            data = data,\n",
    "            matrix = matrix,\n",
    "            cellmap = cellmap,\n",
    "            assaymap = assaymap)\n",
    "\n",
    "start = timer()\n",
    "iter_trained = model2.train(500) # train for 5000 iterations\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "\n",
    "model2_results = model2.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results['preds_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUND_results = os.path.join(tf_epitome_results_dir, \"K562_JUND_motif_anchor.npz\")\n",
    "JUND_preds = np.load(JUND_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUND_preds['pred'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Early Stop Model (Multi TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs = ['ZNF384',\n",
    " 'ZNF274',\n",
    " 'ZNF24',\n",
    " 'ZNF143',\n",
    " 'ZKSCAN1',\n",
    " 'ZFP36',\n",
    " 'ZBTB40',\n",
    " 'ZBTB33',\n",
    " 'YY1',\n",
    " 'YBX1',\n",
    " 'USF2',\n",
    " 'USF1',\n",
    " 'TCF7L2',\n",
    " 'TCF12',\n",
    " 'TBP',\n",
    " 'TARDBP',\n",
    " 'TAF1',\n",
    " 'SUZ12',\n",
    " 'SRF',\n",
    " 'SP1',\n",
    " 'SMC3',\n",
    " 'SIN3A',\n",
    " 'RNF2',\n",
    " 'RFX5',\n",
    " 'REST',\n",
    " 'RCOR1',\n",
    " 'RAD51',\n",
    " 'RAD21',\n",
    " 'POLR2AphosphoS5',\n",
    " 'POLR2AphosphoS2',\n",
    " 'POLR2A',\n",
    " 'PKNOX1',\n",
    " 'PHF8',\n",
    " 'NRF1',\n",
    " 'NR2C2',\n",
    " 'NFRKB',\n",
    " 'NFE2L2',\n",
    " 'NBN',\n",
    " 'MYC',\n",
    " 'MXI1',\n",
    " 'MAZ',\n",
    " 'MAX',\n",
    " 'MAFK',\n",
    " 'KDM1A',\n",
    " 'JUND',\n",
    " 'JUN',\n",
    " 'HDGF',\n",
    " 'HDAC2',\n",
    " 'HCFC1',\n",
    " 'H4K20me1',\n",
    " 'H3K9me3',\n",
    " 'H3K9me2',\n",
    " 'H3K9ac',\n",
    " 'H3K79me2',\n",
    " 'H3K4me3',\n",
    " 'H3K4me2',\n",
    " 'H3K4me1',\n",
    " 'H3K36me3',\n",
    " 'H3K27me3',\n",
    " 'H3K27ac',\n",
    " 'H3F3A',\n",
    " 'H2AFZ',\n",
    " 'GTF2F1',\n",
    " 'GABPA',\n",
    " 'FOXK2',\n",
    " 'FOXA1',\n",
    " 'FOS',\n",
    " 'EZH2phosphoT487',\n",
    " 'EZH2',\n",
    " 'ETS1',\n",
    " 'ESRRA',\n",
    " 'EP300',\n",
    " 'ELK1',\n",
    " 'ELF1',\n",
    " 'CTCF',\n",
    " 'CHD2',\n",
    " 'CHD1',\n",
    " 'CEBPB',\n",
    " 'BRCA1',\n",
    " 'BHLHE40',\n",
    " 'ATF7',\n",
    " 'ATF3',\n",
    " 'ATF2',\n",
    " 'ARNT',\n",
    " 'ARID3A']\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%b%Y_%H%M\")\n",
    "\n",
    "if len(TFs) > 1:\n",
    "    print(TFs)\n",
    "    # Generate a new prefix with the time stamp\n",
    "    TF_names = \"ALL_TFS\"\n",
    "#     prefix = prefix + \"_\" + timestampStr\n",
    "    \n",
    "    # matrix and cellmaps from feature file\n",
    "    feature_path = os.path.join(epitome_data_path, \"feature_name\")\n",
    "    matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = TFs,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VALID_ITERS = 100\n",
    "model_check_p = TF_names + \"_modelcheck\"\n",
    "model_path = TF_names + \"_model\"\n",
    "\n",
    "model = VLP(TFs,\n",
    "            data = data,\n",
    "            matrix = matrix,\n",
    "            cellmap = cellmap,\n",
    "            assaymap = assaymap) #,\n",
    "#             max_valid_records = TRAIN_VALID_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ITERS, TEST_ITERS, patience, min_delta = 500, 1000, 2, 0.1\n",
    "start = timer()\n",
    "# Number of iterations actually trained\n",
    "best_train_iters, tot_train_iters, valid_losses = model.train(TRAIN_ITERS) #,\n",
    "#                                                               checkpoint_path = model_check_p,\n",
    "#                                                               patience = patience,\n",
    "#                                                               min_delta = min_delta)\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "\n",
    "model_results = model.test(TEST_ITERS, calculate_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# dict = {'Python' : '.py', 'C++' : '.cpp', 'Java' : '.java'}\n",
    "f = open(\"file.pkl\",\"wb\")\n",
    "pickle.dump(model_results, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('file.pkl', 'rb')\n",
    "mydict2 = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ITERS, TEST_ITERS, patience, min_delta = 500, 1000, 2, 0.1\n",
    "start = timer()\n",
    "# Number of iterations actually trained\n",
    "best_train_iters, tot_train_iters, valid_losses = model.train(TRAIN_ITERS) #,\n",
    "#                                                               checkpoint_path = model_check_p,\n",
    "#                                                               patience = patience,\n",
    "#                                                               min_delta = min_delta)\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "model.save(model_path)\n",
    "\n",
    "# Test Early Stopping Model\n",
    "model_results = model.test(TEST_ITERS, calculate_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = VLP(checkpoint= model_check_p)\n",
    "best_model_results = best_model.test(TEST_ITERS, calculate_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/jahnavis/epitome_new/epitome-1/epitome/functions.py:311: UserWarning: min_assays_per_cell should not be < 2 (this means it only has a similarity assay) but was set to 1\n",
      "  warnings.warn(\"min_assays_per_cell should not be < 2 (this means it only has a similarity assay) but was set to %i\" % min_assays_per_cell)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligible_cells = ['K562','HepG2','H1','A549','HeLa-S3']\n",
    "eligible_assays = ['DNase','CTCF']\n",
    "similarity_assays = ['DNase']\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file(eligible_assays = eligible_assays,\n",
    "                                                         similarity_assays = similarity_assays,\n",
    "                                                         eligible_cells = eligible_cells, \n",
    "                                                         min_cells_per_assay = 3, \n",
    "                                                         min_assays_per_cell = 1)\n",
    "matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[249, 120],\n",
       "       [396, 398],\n",
       "       [364, 276],\n",
       "       [535, 114],\n",
       "       [614, 701]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((5,2)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VLP(list(eligible_assays),\n",
    "\t\t\ttest_celltypes = ['K562'],\n",
    "\t\t\tmatrix = matrix, #np.ones((5,2)).astype(int),\n",
    "\t\t\tassaymap = assaymap,\n",
    "\t\t\tcellmap = cellmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = 200\n",
    "validation_size = 10\n",
    "\n",
    "# create model and train\n",
    "best_model_steps, num_steps, train_valid_losses = model.train(1)\n",
    "results1 = model.test(validation_size)\n",
    "best_model_steps2, num_steps2, train_valid_losses2 = model.train(train_iters)\n",
    "results2 = model.test(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure predictions are not random\n",
    "# after first iterations\n",
    "assert(results1['preds_mean'].shape[0] == validation_size)\n",
    "assert(results2['preds_mean'][0] < results1['preds_mean'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_steps, best_model_steps2, num_steps, num_steps2, train_valid_losses, train_valid_losses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['HepG2', 'HeLa-S3', 'H1', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['HepG2', 'HeLa-S3', 'H1', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['HepG2', 'HeLa-S3', 'H1', 'A549'] as labels for mode Dataset.VALID\n",
      "using ['K562'] as labels for mode Dataset.TEST\n",
      "WARNING:tensorflow:From /home/eecs/jahnavis/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Starting Training\n",
      "WARNING:tensorflow:AutoGraph could not transform <function VariationalPeakModel.train.<locals>.loopiter at 0x7f9a4403b0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function VariationalPeakModel.train.<locals>.loopiter at 0x7f9a4403b0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function VariationalPeakModel.train.<locals>.train_step at 0x7f9a440d2d90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function VariationalPeakModel.train.<locals>.train_step at 0x7f9a440d2d90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:0 Tensor(\"Mean:0\", shape=(), dtype=float32)Tensor(\"Mean_1:0\", shape=(), dtype=float32)Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "step_v: 0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function VariationalPeakModel.train.<locals>.valid_step at 0x7f9a440d2f28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function VariationalPeakModel.train.<locals>.valid_step at 0x7f9a440d2f28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "step_v: 1\n",
      "step_v: 2\n",
      "step_v: 3\n",
      "step_v: 4\n",
      "step_v: 5\n",
      "step_v: 6\n",
      "step_v: 7\n",
      "step_v: 8\n",
      "step_v: 9\n",
      "step_v: 10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    /home/eecs/jahnavis/epitome_new/epitome-1/epitome/models.py:359 loopiter\n        new_valid_loss = int(tf.concat(new_valid_loss, axis=0))\n\n    TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-61ac495d17d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# create model and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbest_model_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_valid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/epitome_new/epitome-1/epitome/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_steps, patience, min_delta)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_valid_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloopiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 497\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    /home/eecs/jahnavis/epitome_new/epitome-1/epitome/models.py:359 loopiter\n        new_valid_loss = int(tf.concat(new_valid_loss, axis=0))\n\n    TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\n"
     ]
    }
   ],
   "source": [
    "model = VLP(list(eligible_assays),\n",
    "\t\t\ttest_celltypes = ['K562'],\n",
    "\t\t\tmatrix = matrix, #np.ones((5,2)).astype(int),\n",
    "\t\t\tassaymap = assaymap,\n",
    "\t\t\tcellmap = cellmap,\n",
    "            max_valid_records=10)\n",
    "train_iters = 200\n",
    "validation_size = 10\n",
    "\n",
    "# create model and train\n",
    "best_model_steps, num_steps, train_valid_losses = model.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = model.test(validation_size)\n",
    "best_model_steps2, num_steps2, train_valid_losses2 = model.train(train_iters)\n",
    "results2 = model.test(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_steps, best_model_steps2, num_steps, num_steps2, train_valid_losses, train_valid_losses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EpitomeNewEnv2",
   "language": "python",
   "name": "epitomenewenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
