{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/jahnavis/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from epitome.models import *\n",
    "from epitome.functions import *\n",
    "from epitome.viz import *\n",
    "\n",
    "from epitome.constants import *\n",
    "import yaml\n",
    "import subprocess\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "epitome_data_path = \"data/epitome_data\" \n",
    "feature_path = os.path.join(epitome_data_path, \"feature_name\")\n",
    "TF = \"USF2\"\n",
    "query_cell = 'K562' #'T47D'\n",
    "prefix = \"cons_dec\" # consecutive decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user directories if they do not exist\n",
    "epitome_results_dir = os.path.join(results_path, \"epitome_results\")\n",
    "if not os.path.exists(epitome_results_dir):\n",
    "    os.makedirs(epitome_results_dir)\n",
    "    \n",
    "tf_epitome_results_dir = os.path.join(epitome_results_dir, TF + \"_\" + prefix + \"_results\")\n",
    "if not os.path.exists(tf_epitome_results_dir):\n",
    "    os.makedirs(tf_epitome_results_dir)\n",
    "    \n",
    "model_dir = os.path.join(results_path, \"epitome_models\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "tf_model_dir = os.path.join(model_dir, TF +  \"_\" + prefix + \"_models\")\n",
    "if not os.path.exists(tf_model_dir):\n",
    "    os.makedirs(tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data for Epitome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'train.npz')).toarray()\n",
    "valid_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'valid.npz')).toarray()\n",
    "test_data = scipy.sparse.load_npz(os.path.join(epitome_data_path, 'test.npz')).toarray()\n",
    "data = {Dataset.TRAIN: train_data, Dataset.VALID: valid_data, Dataset.TEST: test_data}\n",
    "# all_data = np.concatenate((data[Dataset.TRAIN], data[Dataset.VALID], data[Dataset.TEST]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model with Multiple TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_overlap_tfs = pd.read_csv(\n",
    "    \"/home/eecs/jahnavis/epitome_new/epitome-1/data/epitome_data/Anchor_Epitome_Overlap_TFs.csv\")['TF'].tolist()\n",
    "anchor_tfs = [\"CTCF\", \"E2F1\", \"EGR1\", \"FOXA1\", \"FOXA2\", \"GABPA\", \"HNF4A\", \"JUND\", \n",
    "              \"MAX\", \"NANOG\", \"REST\", \"TAF1\"]\n",
    "# anchor_overlap_tfs = set(epitome_tfs).intersection(set(anchor_tfs))\n",
    "# len(anchor_tfs), len(anchor_overlap_tfs), anchor_overlap_tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = anchor_overlap_tfs,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n",
    "VLP(anchor_overlap_tfs,\n",
    "    data = data,\n",
    "    matrix = matrix,\n",
    "    cellmap = cellmap,\n",
    "    assaymap = assaymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs = [\"CEBPB\", \"CHD2\", \"CTCF\", \"EP300\", \"GABPA\", \"JUND\", \"MAFK\", \"MAX\", \n",
    "       \"MYC\", \"NRF1\", \"RAD21\", \"REST\", \"RFX5\", \"SRF\", \"TAF1\", \"TBP\", \"USF2\"]\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = TFs,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n",
    "VLP(TFs,\n",
    "    data = data,\n",
    "    matrix = matrix,\n",
    "    cellmap = cellmap,\n",
    "    assaymap = assaymap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model with Early Stopping (Single TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.VALID\n",
      "INFO:tensorflow:Starting Training\n",
      "INFO:tensorflow:0 tf.Tensor(46.807243, shape=(), dtype=float32)tf.Tensor(46.795624, shape=(), dtype=float32)tf.Tensor(0.011619375, shape=(), dtype=float32)\n",
      "INFO:tensorflow:0 Validation Generator Time: 217.7490125373006 seconds\n",
      "INFO:tensorflow:0 Validation:tf.Tensor(45.24661, shape=(), dtype=float32)\n",
      "INFO:tensorflow:\n",
      "epitome train: 220.723507\n"
     ]
    }
   ],
   "source": [
    "max_valid_iterations= 1000\n",
    "\n",
    "model = VLP([TF], \n",
    "            max_valid_records=max_valid_iterations)\n",
    "\n",
    "start = timer()\n",
    "model_checkpoint_path = os.path.join(tf_model_dir, TF + \"_test_earlystop_model_checkpoint\")\n",
    "best_iters_trained, actual_iters_trained, valid_losses = model.train(2, checkpoint_path = model_checkpoint_path)\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "# model_path = os.path.join(tf_model_dir, TF + \"_early_stop_\" + str(iters_trained) + \"_\" + str(max_valid_iterations))\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [01:04,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:macro auROC:     0.9969883234872206\n",
      "INFO:tensorflow:auPRC:     0.6081196403616143\n",
      "INFO:tensorflow:GINI:     0.9939766243796371\n",
      "Model auROC: 0.9969883234872206. Model auPRC: 0.6081196403616143.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_results = model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'IMR-90', 'HepG2', 'HeLa-S3', 'H1', 'GM12878', 'A549'] as labels for mode Dataset.VALID\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_model = VLP(checkpoint=model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [01:06,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:macro auROC:     0.997354660579044\n",
      "INFO:tensorflow:auPRC:     0.590895647096396\n",
      "INFO:tensorflow:GINI:     0.9947092910316824\n",
      "Model auROC: 0.997354660579044. Model auPRC: 0.590895647096396.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_results = model_checkpoint_model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_checkpoint_results['auROC'], model_checkpoint_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=35.49988>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.34261>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.11333>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.866198>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.798954>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.6424932>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.615142>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.87403>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.5501018>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.5447745>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.628159>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.5731063>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['valid_losses'])\n",
    "eval_results_df = eval_results_df.append({ 'valid_losses':valid_losses}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      tf.Tensor(35.49988, shape=(), dtype=float32)\n",
       "1       tf.Tensor(6.34261, shape=(), dtype=float32)\n",
       "2       tf.Tensor(6.11333, shape=(), dtype=float32)\n",
       "3      tf.Tensor(5.866198, shape=(), dtype=float32)\n",
       "4      tf.Tensor(5.798954, shape=(), dtype=float32)\n",
       "5     tf.Tensor(6.6424932, shape=(), dtype=float32)\n",
       "6      tf.Tensor(5.615142, shape=(), dtype=float32)\n",
       "7       tf.Tensor(5.87403, shape=(), dtype=float32)\n",
       "8     tf.Tensor(5.5501018, shape=(), dtype=float32)\n",
       "9     tf.Tensor(5.5447745, shape=(), dtype=float32)\n",
       "10     tf.Tensor(5.628159, shape=(), dtype=float32)\n",
       "11    tf.Tensor(5.5731063, shape=(), dtype=float32)\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(valid_losses).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['transcription_factor', 'query_cell', 'auROC', 'auPRC'])\n",
    "eval_results_df = eval_results_df.append({ \n",
    "   'transcription_factor' : TF,\n",
    "   'query_cell' : query_cell,\n",
    "   'auROC' : model_results['auROC'],\n",
    "   'auPRC' : model_results['auPRC'],\n",
    "   'iterations_trained' : iter_trained,\n",
    "   'train_time': train_time}, \n",
    "    ignore_index=True)\n",
    "\n",
    "eval_results_dir = os.path.join(tf_epitome_results_dir, query_cell + \"_\" + TF + \n",
    "                           '_no_motif_early_stop_'+ str(max_valid_iterations) + \n",
    "                           '.csv')\n",
    "eval_results_df.to_csv(eval_results_dir, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLP Model Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "#                                                          eligible_assays = None,\n",
    "#                                                          eligible_cells = None, \n",
    "#                                                          min_cells_per_assay = 2, \n",
    "#                                                          min_assays_per_cell= 2) #10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['K562', 'HepG2', 'HeLa-S3', 'HCT116', 'H1', 'GM12878'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'HepG2', 'HeLa-S3', 'HCT116', 'H1', 'GM12878'] as labels for mode Dataset.TRAIN\n",
      "using ['K562', 'HepG2', 'HeLa-S3', 'HCT116', 'H1', 'GM12878'] as labels for mode Dataset.VALID\n",
      "WARNING:tensorflow:From /home/eecs/jahnavis/miniconda/envs/EpitomeNewEnv2/lib/python3.6/site-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Starting Training\n",
      "INFO:tensorflow:0 tf.Tensor(42.065647, shape=(), dtype=float32)tf.Tensor(42.05568, shape=(), dtype=float32)tf.Tensor(0.0099677, shape=(), dtype=float32)\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:200 tf.Tensor(3.3761077, shape=(), dtype=float32)tf.Tensor(3.3661175, shape=(), dtype=float32)tf.Tensor(0.009990176, shape=(), dtype=float32)\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:400 tf.Tensor(1.4245248, shape=(), dtype=float32)tf.Tensor(1.4145114, shape=(), dtype=float32)tf.Tensor(0.010013362, shape=(), dtype=float32)\n",
      "INFO:tensorflow:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epitome train: 135.497210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:34,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:macro auROC:     0.9939963978387032\n",
      "INFO:tensorflow:auPRC:     0.17390570108786427\n",
      "INFO:tensorflow:GINI:     0.9879927956774065\n",
      "Model auROC: 0.9939963978387032. Model auPRC: 0.17390570108786427.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TF = \"JUND\"\n",
    "model = VLP([TF])\n",
    "\n",
    "start = timer()\n",
    "iter_trained = model.train(500) # train for 5000 iterations\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "\n",
    "model_results = model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.DataFrame(columns=['transcription_factor', 'query_cell', 'auROC', 'auPRC'])\n",
    "eval_results_df = eval_results_df.append({ \n",
    "   'transcription_factor' : TF,\n",
    "   'query_cell' : query_cell,\n",
    "   'auROC' : model_results['auROC'],\n",
    "   'auPRC' : model_results['auPRC'],\n",
    "   'iterations_trained' : iter_trained,\n",
    "   'train_time': train_time}, \n",
    "    ignore_index=True)\n",
    "\n",
    "eval_results_df.to_csv(os.path.join(tf_epitome_results_dir,\n",
    "                                    query_cell + \"_\" + TF + '_no_motif' + '.csv'), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['preds_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = \"JUND\"\n",
    "\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file(feature_path,\n",
    "                                                         eligible_assays = TF,\n",
    "                                                         eligible_cells = None, \n",
    "                                                         min_cells_per_assay = 2, \n",
    "                                                         min_assays_per_cell= 2)\n",
    "    \n",
    "model2 = VLP([TF],\n",
    "            data = data,\n",
    "            matrix = matrix,\n",
    "            cellmap = cellmap,\n",
    "            assaymap = assaymap)\n",
    "\n",
    "start = timer()\n",
    "iter_trained = model2.train(500) # train for 5000 iterations\n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print('epitome train: %f' % train_time)\n",
    "\n",
    "model2_results = model2.test(10000, calculate_metrics=True)\n",
    "print('Model auROC: %s. Model auPRC: %s.' % (model_results['auROC'], model_results['auPRC'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results['preds_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUND_results = os.path.join(tf_epitome_results_dir, \"K562_JUND_motif_anchor.npz\")\n",
    "JUND_preds = np.load(JUND_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUND_preds['pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EpitomeNewEnv2",
   "language": "python",
   "name": "epitomenewenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
